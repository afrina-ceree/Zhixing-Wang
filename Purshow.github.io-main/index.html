<!DOCTYPE html>
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta content="IE=5.0000" http-equiv="X-UA-Compatible">
    <meta name="description" content="Yuwei Niu's home page">
    <link rel="icon" media="(prefers-color-scheme:dark)" href="./assets/imgs/Luffy.jpg" type="image/jpg" />
    <link rel="icon" media="(prefers-color-scheme:light)" href="./assets/imgs/Luffy.jpg" type="image/jpg" />
<!--     <link rel="icon" media="(prefers-color-scheme:light)" href="./assets/imgs/favicon.png" type="image/png" /> -->
    <link href="./profile.css" rel="stylesheet" type="text/css">
    <title>Yuwei Niu's Homepage</title>
    <meta name="GENERATOR" content="MSHTML 11.00.10570.1001">
        <style>
        body {
            font-family: Georgia, serif;
            color: #111;
            width: 850px;
        }

        .profile-pic {
            width: 180px;
            border-radius: 50%;
            box-shadow: 0 4px 10px rgba(0, 0, 0, 0.1);
            transition: transform 0.3s ease;
        }

        .profile-pic:hover {
            transform: scale(1.04);
        }

        .top-section {
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .contact-box {
            background: #f9f9f9;
            border-radius: 8px;
            padding: 10px 12px;
            display: inline-block;
            margin-top: 0.6em;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.05);
            font-size: 0.95em;
        }

        .contact-box img {
            width: 16px;
            vertical-align: middle;
            margin-right: 6px;
            opacity: 0.9;
        }

        .contact-box a {
            margin-right: 16px;
        }

        .emoji {
            font-family: Apple Color Emoji, Segoe UI Emoji, NotoColorEmoji, sans-serif;
            font-size: 1.1em;
        }

        .publication-entry {
            display: flex;
            align-items: flex-start;
            background: #fdfdfd;
            padding: 12px;
            margin: 15px 0;
            border-radius: 10px;
            border: 1px solid #eee;
            box-shadow: 0 2px 6px rgba(0, 0, 0, 0.03);
        }

        .publication-entry img {
            width: 200px;
            height: 100px;
            border-radius: 8px;
            margin-right: 16px;
            object-fit: cover;
        }

        .pub-list {
            list-style: none;
            padding-left: 0;
        }

        h1,
        h2 {
            color: #263054;
        }

        h2 {
            border-bottom: 1px solid #aaa;
            padding-bottom: 5px;
            margin-top: 20px;
        }
    </style>
</head>

<body>


    <div id="layout-content" style="margin: 25px; max-width: 800px;">
        <div class="top-section">
            <div style="max-width: 70%;">
                <h1>Yuwei Niu</h1>
                <h3><span class="emoji">üéì</span> Undergraduate Student</h3>

                <p style="margin-bottom: 0.4em;">
                    <img src="assets/imgs/school.png" alt="University"
                        style="width:16px; vertical-align:middle; margin-right:5px;">
                    <a href="https://www.cqu.edu.cn/" target="_blank">Chongqing University</a>
                </p>
                <p style="margin-top: 0;">
                    <img src="assets/imgs/location.png" alt="Location"
                        style="width:16px; vertical-align:middle; margin-right:5px;">
                    Chongqing, China
                </p>

                <div class="contact-box">
                    <img src="assets/imgs/envelope.png" alt="Email">
                    <a href="mailto:niuyuwei04@gmail.com">niuyuwei04@gmail.com</a>

                    <img src="assets/imgs/google.png" alt="Google Scholar">
                    <a href="https://scholar.google.com/citations?user=VaYRTxEAAAAJ&hl=zh-CN&oi=ao" target="_blank">Google Scholar</a>

                    <img src="assets/imgs/github.png" alt="GitHub">
                    <a href="https://github.com/purshow" target="_blank">GitHub</a>
                </div>
            </div>
            <div>
                <img class="profile-pic" src="./assets/imgs/Luffy.jpg" alt="Profile Picture">
            </div>
        </div>

    
<!--     <div id="layout-content" style="margin-top: 25px;">
        <table>
            <tbody>
                <tr>
                    <td width="670">
                        <div id="toptitle">
                            <h1>Yuwei Niu</h1>
                        </div>
                        <h3>üéì Undergraduate Student</h3>
                        <p>
                            <a href="https://www.cqu.edu.cn/">Chongqing University</a>,
                            <br>Chongqing, China
                            <br>
                            <br> Email:
                            <a href="mailto:niuyuwei04@gmail.com">niuyuwei04@gmail.com</a>
                            <br> WeChat: purshow
                            <br>
                            <br>
                            [<a href="https://scholar.google.com/citations?user=VaYRTxEAAAAJ&hl=zh-CN&oi=ao">Google Scholar</a>]&nbsp;&nbsp;[<a href="https://github.com/purshow">GitHub</a>]
                        </p>
                    </td>
                    <td>
                        <div>
                            <img width="270" src="./assets/imgs/Luffy.jpg" border="0">
                        </div>
                    </td>
                </tr>
                <tr></tr>
            </tbody>
        </table> -->

        <div id="layout-content" style="margin-top: 25px;">
            <h2>About Me</h2>
            <p>
I am a <b>fourth-year</b> undergraduate student in the <b>Hongshen Stand-Out Class of Computer Science</b> at <b>Chongqing University</b>. I am fortunate to have closely collaborated with <b>Prof. <a href="https://scholar.google.com/citations?user=KomQOFkAAAAJ&hl=zh-CN&oi=ao">Lei Feng</a></b> during my undergraduate studies and will be joining <b>Peking University</b>, advised by <b>Prof. <a href="https://scholar.google.com/citations?user=-5juAR0AAAAJ&hl=zh-CN">Li Yuan</a></b>.            </p>
            <p>
My core research focuses on visual representations and language priors. Vision directly depicts the physical appearance of the "real world"; while language, as the refinement and abstraction of visual information, constructs the knowledge landscape of the "human world." I firmly believe that bridging the gap between the language modality, which carries "human world" prior knowledge, and the visual modality, which directly reflects the "real world," is a crucial step towards achieving general artificial intelligence. This not only involves finding effective mapping paths between modalities but also concerns how to leverage their respective strengths to build a unified framework collaboratively.            </p>
            </p>
            <p>
                Currently, I am dedicated to building unified multimodal models and exploring why such models are essential.
            </p>
            <p>
               ü§ù I am eager to discuss potential collaborations and am actively seeking industry internship opportunities in multimodal models. Please feel free to contact me via email or  <b>WeChat: purshow</b> if you are interested.
            </p>
            <h2>Education</h2>
            <ul>
                <li>
                    [2022-2026] B.Eng. in Computer Science, <b>Hongshen Stand-Out Class</b>, Chongqing University.
                </li>
            </ul>
<!-- 
            <h2>Research Experience</h2>
            <ul>
                <li>
                    [Dec 2024 - Now] Research Intern, Bytedance Seed
                    <br>Researching on building unified multimodal large models, supervised by Researcher Chaoran Guo.
                </li>
                <li>
                    [Dec 2024 - Now] Research Assistant, Peking University.
                    <br>Researching on building unified multimodal large models, supervised by Researcher <a href="https://scholar.google.com/citations?user=-5juAR0AAAAJ">Li Yuan</a>.
                </li>
                <li>
                    [Oct 2023 - Dec 2024] Research Assistant, Nanyang Technological University.
                    <br>Researching on building trustworthy multimodal models, supervised by Prof. <a href="https://scholar.google.com/citations?user=KomQOFkAAAAJ">Lei Feng</a>.
                </li>
            </ul> -->

<h2>Selected Publications & Manuscripts</h2>
            * Equal Contribution &nbsp;&nbsp; # Project Lead

            <ul>
                <li style="margin-top: 0px;">
                    <div style="margin-top: 5px;"><b>üî• WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation</b></div>
                    <div style="margin-top: 0px;"><a style="color: #777;"><u><b>Yuwei Niu*</b></u>, Munan Ning*, Mengren Zheng, Weiyang Jin, Bin Lin, Peng Jin, Jiaqi Liao, Chaoran Feng, Kunpeng Ning, Bin Zhu, Li Yuan‚Ä†.</a></div>
                    <div style="margin-top: 0px;"><a style="color: #b70505c0;"><i>Probing Language Priors in Multimodal Unify Models. First to explore how world knowledge can benefit the generation of unified models. Widely adopted by many prominent researchers.</i></a></div>
                    <div style="margin-top: 2px;">
                        <i><b>Under Review</b></i> &nbsp;
                        [<a href="https://arxiv.org/abs/2503.07265">Paper</a>]
                        [<a href="https://github.com/PKU-YuanGroup/WISE">Code üåü150+</a>]
                    </div>
                </li>
            </ul>

            <ul>
                <li style="margin-top: 0px;">
                    <div style="margin-top: 5px;"><b>üî• LangBridge: Interpreting Image as a Combination of Language Embeddings</b></div>
                    <div style="margin-top: 0px;"><a style="color: #777;">Jiaqi Liao*, <u><b>Yuwei Niu*</b></u>, Fanqing Meng*, Hao Li, Changyao Tian, Yinuo Du, Yuwen Xiong, Dianqi Li, Xizhou Zhu, Li Yuan, Jifeng Dai‚Ä†, Yu Cheng‚Ä†.</a></div>
                    <div style="margin-top: 0px;"><a style="color: #b70505c0;"><i>Language Priors as Visual Representations. Introducing a novel multimodal alignment paradigm that competes with LLaVA.</i></a></div>
                    <div style="margin-top: 2px;">
                        <i><b>ICCV 2025</b></i> &nbsp;
                        [<a href="https://arxiv.org/abs/2503.19404">Paper</a>]
                        [<a href="https://jiaqiliao77.github.io/LangBridge.github.io/">Code</a>]
                    </div>
                </li>
            </ul>

            <ul>
                <li style="margin-top: 0px;">
                    <div style="margin-top: 5px;"><b>üî• UniWorld: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation</b></div>
                    <div style="margin-top: 0px;"><a style="color: #777;">Bin Lin, Zongjian Li, Xinhua Cheng, <u><b>Yuwei Niu</b></u>, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, Yatian Pang, Li Yuan‚Ä†.</a></div>
                    <div style="margin-top: 0px;"><a style="color: #b70505c0;"><i>A comprehensive and powerful unified model!</i></a></div>
                    <div style="margin-top: 2px;">
                        <i><b>Tech Report</b></i> &nbsp;
                        [<a href="https://arxiv.org/abs/2506.03147">Paper</a>]
                        [<a href="https://github.com/PKU-YuanGroup/UniWorld-V1">Code üåü700+</a>]
                    </div>
                </li>
            </ul>

<h2>Other Works</h2>
<ul>
    <li style="margin-top: 15px;">
        <div style="margin-top: 5px;"><b>Test-Time Multimodal Backdoor Detection by Contrastive Prompting</b></div>
        <div style="margin-top: 0px;"><a style="color: #777;"><u><b>Yuwei Niu*</b></u>, Shuo He*, Qi Wei, Zongyu Wu, Feng Liu, Lei Feng‚Ä†.</a></div>
        <div style="margin-top: 2px;">
            <i><b>ICML 2025</b></i> &nbsp;
            [<a href="https://arxiv.org/abs/2405.15269">Paper</a>]
        </div>
    </li>

    <li style="margin-top: 15px;">
        <div style="margin-top: 5px;"><b>ICT: Image-Object Cross-Level Trusted Intervention for Mitigating Object Hallucination in Large Vision-Language Models</b></div>
        <div style="margin-top: 0px;"><a style="color: #777;">Junzhe Chen, Tianshu Zhang, Shiyu Huang, <u><b>Yuwei Niu</b></u>, Linfeng Zhang, Lijie Wen, Xuming Hu‚Ä†.</a></div>
        <div style="margin-top: 2px;">
            <i><b>CVPR 2025</b></i> &nbsp;
            [<a href="https://arxiv.org/abs/2411.15268">Paper</a>]
        </div>
    </li>

    <li style="margin-top: 15px;">
        <div style="margin-top: 5px;"><b>Tuning Vision-Language Models with Candidate Labels by Prompt Alignment</b></div>
        <div style="margin-top: 0px;"><a style="color: #777;">Zhifang Zhang, <u><b>Yuwei Niu</b></u>, Xin Liu, Beibei Li‚Ä†.</a></div>
        <div style="margin-top: 2px;">
            <i><b>DASFAA 2025</b></i> &nbsp;
            [<a href="https://arxiv.org/abs/2407.07638">Paper</a>]
        </div>
    </li>

    <li style="margin-top: 15px;">
        <div style="margin-top: 5px;"><b>Look-Back: Implicit Visual Re-focusing in MLLM Reasoning</b></div>
        <div style="margin-top: 0px;"><a style="color: #777;">Shuo Yang*, <u><b>Yuwei Niu*</b></u>, Yuyang Liu‚Ä†, Yang Ye, Bin Lin, Li Yuan‚Ä†.</a></div>
        <div style="margin-top: 2px;">
                        <i><b>Under Review</b></i> &nbsp;
            [<a href="https://arxiv.org/abs/2507.03019">Paper</a>]
        </div>
    </li>

    <li style="margin-top: 15px;">
        <div style="margin-top: 5px;"><b>LanP: Rethinking the Impact of Language Priors in Large Vision-Language Models</b></div>
        <div style="margin-top: 0px;"><a style="color: #777;">Zongyu Wu*, <u><b>Yuwei Niu*</b></u>, Hongcheng Gao, Minhua Lin, Zhiwei Zhang, Zhifang Zhang, Qi Shi, Yilong Wang, Sike Fu, Junjie Xu, Junjie Ao, Enyan Dai, Lei Feng, Xiang Zhang, Suhang Wang‚Ä†.</a></div>
        <div style="margin-top: 2px;">
                        <i><b>Under Review</b></i> &nbsp;
            [<a href="https://arxiv.org/abs/2502.12359">Paper</a>]
        </div>
    </li>

    <li style="margin-top: 15px;">
        <div style="margin-top: 5px;"><b>OMNIDPO: A Preference Optimization Framework to Address Omni-Modal Hallucination</b></div>
        <div style="margin-top: 0px;"><a style="color: #777;">Junzhe Chen, Tianshu Zhang, Shiyu Huang, <u><b>Yuwei Niu</b></u>, Rongzhou Zhang, Guanyu Zhou, Lijie Wen, Xuming Hu‚Ä†.</a></div>
        <div style="margin-top: 2px;">
                        <i><b>Under Review</b></i> &nbsp;
            [<a href="https://arxiv.org/abs/2509.00723">Paper</a>]
        </div>
    </li>
</ul>
            <h2>Community Contribution</h2>
            <ul>
                <li>
                    <b>Founder of <a href="https://github.com/Purshow/Awesome-Unified-Multimodal">Awesome-Unified-Multimodal</a></b>
                    <br>Created the most comprehensive repository for organizing papers, codes, and resources on unified multimodal models. (<a href="https://github.com/Purshow/Awesome-Unified-Multimodal">GitHub üåü300+</a>)
                </li>
            </ul>

            <h2>Personality</h2>
            <p>
Beyond science, I am deeply passionate about literature, poetry, anime, films, music, and all forms of art that embody human creativity. I firmly believe these are the reasons for our existence. Currently, I am deeply captivated by the literature of Dostoevsky and Tolstoy, and R&B music (such as Stevie Wonder, Prince, David Tao, and Khalil Fong).            </p>


            <h2>Contact Me</h2>
            <p>
I truly believe that great ideas and improvements come from open discussions and debates in academia. If you have any thoughts, disagreements with my work, or fresh ideas you‚Äôd like to share, I‚Äôd be really grateful to hear from you. I am incredibly fortunate to have met many friends who have helped me along the way, and in turn, I am always willing to chat and offer any assistance I can to others.

            </p>
            <p>
                If you‚Äôve got any questions about my research or if you‚Äôve tried reaching out through GitHub issues and haven‚Äôt heard back, please don‚Äôt hesitate to drop me an email.
            </p>
            <p>
                Please note that I am only interested in discussing intriguing problems and insights, not metrics. If you are inclined to discuss publication or citation numbers, rely on numerical indicators to quantify individuals, or compare me to others, please refrain from contacting me.
            </p>
            <p>My preferred email: <a href="mailto:niuyuwei04@gmail.com">niuyuwei04@gmail.com</a></p>


        </div>
    </div>

</body>
</html>
